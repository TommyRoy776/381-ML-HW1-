{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d026d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSCI 381/780 (Fall 2022) - Project 1\n",
    "\n",
    "**Due Date: Friday, October 7 by 4 PM**\n",
    "\n",
    "## Description\n",
    "In this project you will construct machine learning models on a dataset comprised of sociodemographic data of U.S. citizens collected from the 1994 U.S. Census. The machine learning task will be binary classification of people based on their income level being either <= 50,000 USD or > 50,000 USD.\n",
    "\n",
    "## Instructions\n",
    "1. In this project you will write code to construct machine learning models using various partitions (see the figure below) of the census dataset and write responses to questions concerning the performance of said models. Please complete all sections below, adding new *Code* or *Markdown* cells as appropriate to answer the questions.\n",
    "2. There are many Scikit-learn functions that leverage randomness to generate results. For these functions, a pseudorandom generator can be intialized using a seed value by passing the parameter `random_state=XXX`, where `XXX` is some number between 1 and 2^31-1. For each of these functions, **you will utilize your CUNY ID number** to initialize the function. Functions include:\n",
    "- `StratifiedShuffleSplit`\n",
    "- `RandomForestClassifier`\n",
    "- `RandomizedSearchCV`\n",
    "3. You will **work independently** on the project. Please make use of the *Python Data Science Reference Materials* posted on Blackboard, or come to office hours should you need further assistance.\n",
    "4. You will submit a single Jupyter notebook containing all code and written responses via Blackboard by the due date listed above. \n",
    "\n",
    "<img src=\"project-1-data-folds.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "## Grading\n",
    "\n",
    "### Running Code\n",
    "Your Jupyter notebook must be able to run from start to finish **without error**. Please turn any cell that contains scratch work or other non-executable items to *Raw*. **Notebooks that cannot run to completion will receive a grade of 0**.\n",
    "\n",
    "### Holdout Set Evaluation\n",
    "Your final models will be evaluated against a holdout set. You model performances with respect to AUC on this set must be *comparable* (e.g., within 5%) of those reported in Part 6.\n",
    "\n",
    "### Rubric\n",
    "\n",
    "|**Part**|0|1|2|3|4|5|6|7|**Total**|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**%**|15|5|10|20|10|10|20|10|100|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2a56d",
   "metadata": {},
   "source": [
    "## Part 0: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable `CENSUS_FILE` to the **full path** to the census dataset (**census_dataset.csv**) on your system. Load the file into a dataframe (you may intialize the column names using the header list `census_column_names`), then:\n",
    "1. Determine the number and types of features.\n",
    "2. Determine the number of classes and their prevalence in the dataset. Are the classes balanced?\n",
    "3. Perform any necessary preprocessing on dataset.\n",
    "4. Perform a **stratified split** of the data into training/validation/test sets, 60%/20%/20%. \n",
    "5. Verify that the training/validation/test splits have the same prevalence as the original dataset.\n",
    "6. Standardize the training/validation/test splits (fit on the training, then transform the validation/test sets). Use the standardized splits for the SVM models *only*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_column_names=['age','workclass','fnlwgt','education','education-num',\n",
    "                      'marital-status','occupation','relationship','race','sex',\n",
    "                      'capital-gain','capital-loss','hours-per-week','native-country',\n",
    "                      'annual-income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1a3c8",
   "metadata": {},
   "source": [
    "## Part 1: Train Initial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20cf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using default hyperparameters:\n",
    "1. Construct **Naive Bayes (NB)**, **Support Vector Machine (SVM)** , and **Random Forest (RF)** models on the training set.\n",
    "2. Calculate the confusion matrix and report the following performance metrics on the **training set**:\n",
    "    *Accuracy*, *F1 Score*, *AUC*, *Sensitivity*, *Specificity*, and *Precision*. You can use the function `p1_metrics` for this purpose. Are any of the models underfitting the data? Is so, why?\n",
    "\n",
    "3. Calculate the same metrics by applying the trained model to the **validation set**. Compare and contrast the errors each model makes in terms of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peformance metric functions\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "#A list of keys for the dictionary returned by p1_metrics\n",
    "metric_keys = ['auc','f1','accuracy','sensitivity','specificity', 'precision']\n",
    "\n",
    "def p1_metrics(y_true,y_pred,include_cm=True):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    if include_cm:\n",
    "        return {\n",
    "            'auc': roc_auc_score(y_true,y_pred),\n",
    "            'f1': f1_score(y_true,y_pred),\n",
    "            'accuracy': (tp+tn)/np.sum(cm),\n",
    "            'sensitivity': tp/(tp+fn),\n",
    "            'specificity': tn/(tn+fp),\n",
    "            'precision': tp/(tp+fp),\n",
    "            'confusion_matrix': cm}\n",
    "    else:\n",
    "        return {\n",
    "            'auc': roc_auc_score(y_true,y_pred),\n",
    "            'f1': f1_score(y_true,y_pred),\n",
    "            'accuracy': (tp+tn)/np.sum(cm),\n",
    "            'sensitivity': tp/(tp+fn),\n",
    "            'specificity': tn/(tn+fp),\n",
    "            'precision': tp/(tp+fp)}\n",
    "\n",
    "#This wrapper can be used to return multiple performance metrics during cross-validation\n",
    "def p1_metrics_scorer(clf,X,y_true):\n",
    "    y_pred=clf.predict(X)\n",
    "    return p1_metrics(y_true,y_pred,include_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784cab5",
   "metadata": {},
   "source": [
    "## Part 2: Cross-Validation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split the **non-test data** (*training* + *validation* data) into **stratified 5-folds** for cross-validation purposes, then:\n",
    "1. Train NB, SVM, and RF models using 5-fold cross-validation.\n",
    "2. Report the mean and standard deviation of the performance metrics listed in Part 1.2 for each model. You may use the function `collate_cv_results` for this purpose.\n",
    "3. How does the performance of these models compare with those created in Part 1? Which models' performances are more consistent, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f31b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during cross-validation\n",
    "def collate_cv_results(cv_results,display=True):\n",
    "    cv_stats=dict()\n",
    "    for k in cv_results:\n",
    "        cv_stats[k+\"_mean\"]=np.mean(cv_results[k])\n",
    "        cv_stats[k+\"_std\"]=np.std(cv_results[k])\n",
    "        if display:\n",
    "            print(k,cv_stats[k+\"_mean\"],\"(\"+str(cv_stats[k+\"_std\"])+\")\")\n",
    "    return cv_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee068b",
   "metadata": {},
   "source": [
    "## Part 3: SVM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8496b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utilizing the cross-validation dataset from Part 2, you will construct SVM models using the below strategies:\n",
    "1. Using `GridSearchCV`, determine the best choice of hyperparameters out of the following possible values:\n",
    "- *Kernel type*: Linear, radial basis function\n",
    "- *Box constraint (C)*: [1, 5, 10, 20]\n",
    "- *Kernel width (gamma)*: 'auto','scale'\n",
    "2. Report the time required to perform cross-validation via `GridSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameters. You may use the function `collate_ht_results` for this purpose.\n",
    "3. Using `RandomizedSearchCV`, determine the best choice of hyperparameters after **16 trials** using the same possible values for *kernel type* and *kernel width* but with possible **box constraint** values of [1,5,10,15,....,100]. \n",
    "4. Report the time required to perform cross-validation via `RandomizedSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameters.\n",
    "5. Compare the performance of the two models (using the metrics in 1.2) as well as the time required to compute the models. Which hyperparameter method do you think is better in this case, and why?\n",
    "\n",
    "### Please Read!\n",
    "There are a few parameters for the `GridSearchCV` and `RandomizedSearchCV` functions that should be set:\n",
    "- `scoring` - This controls the strategy to evaluate the performance of the cross-validated model on the test set, set it to `p1_metrics_scorer`.\n",
    "- `refit` - This will refit an estimator using the best found parameters on the whole dataset, set it to `\"auc\"`\n",
    "- `cv` - This will enable you to reuse your CV splits from Part 2.\n",
    "    `n_jobs` - Number of jobs to run in parallel, if you have more than one core on your device (you should), set this to as many as you'd like to use, or to `-1` if you want to use all available cores.\n",
    "- `return_train_score` - Setting this to `False` will reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d86235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during hyperparameter tuning\n",
    "def collate_ht_results(ht_results,metric_keys=metric_keys,display=True):\n",
    "    ht_stats=dict()\n",
    "    for metric in metric_keys:\n",
    "        ht_stats[metric+\"_mean\"] = ht_results.cv_results_[\"mean_test_\"+metric][ht_results.best_index_]\n",
    "        ht_stats[metric+\"_std\"] = metric_std = ht_results.cv_results_[\"std_test_\"+metric][ht_results.best_index_]\n",
    "        if display:\n",
    "            print(\"test_\"+metric,ht_stats[metric+\"_mean\"],\"(\"+str(ht_stats[metric+\"_std\"])+\")\")\n",
    "    return ht_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2f1db",
   "metadata": {},
   "source": [
    "## Part 4: Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca639f6f",
   "metadata": {},
   "source": [
    "Utilizing the cross-validation dataset from Part 2, construct a Random Forest model:\n",
    "\n",
    "1. Using `RandomizedSearchCV`, determine the best choice of hyperparameters after **16 trials** using the following possible values:\n",
    "- *Split criterion*: Gini impurity, information gain (entropy)\n",
    "- *Maximum tree depth*:None, log<sub>2</sub>|cross-validation dataset|-1 (use `ceil` to round up to the nearest integer)\n",
    "- *Number of trees*: [10,20,...,100]\n",
    "2. Report the time required to perform cross-validation via `RandomizedSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameters. You may use the function `collate_ht_results` for this purpose.\n",
    "2. Compare the performance of this model to the best RF model constructed in Part 2. Which performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96964f7a",
   "metadata": {},
   "source": [
    "## Part 5: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perform a stratified split of the **training data** into feature selection (one-sixth)/training (five-sixths) sets, then:\n",
    "1. Calculate feature importance on the feature selection subset using *Random Forest Feature Importance (RFFI)*. Produce a bar graph showing each feature's importance and its standard deviation, as determined by RFFI, in descending order.\n",
    "2. Choosing the **top 6 features**, train NB, SVM, and RF models on this new,smaller training set and test on the validation set.\n",
    "3. Choosing the **top 11 features**, train NB, SVM, and RF models on this new,smaller training set and test on the validation set.\n",
    "4. Compare the performance of the models with respect the algorithm used (e.g., NB) and the number of features chosen (6, 11, and all features (those constructed in Part 1). How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a605ce",
   "metadata": {},
   "source": [
    "## Part 6: Final Models and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Using the full training set (**feature selection + training + validation**), train NB, SVM, and RF models, then apply them to the test set. Your final NB, SVM, and RF models should be named `gnb_final`, `svm_final`, and `rf_final`.\n",
    "2. Create a bar chart of mean metrics from cross-validation, with standard deviation as an error bar for each model on the same plot. Use the NB model from Part 2, but the optimal models from Parts 3 and 4 for SVM and RF respectively.\n",
    "3. Plot Receiver Operating Characteristic (ROC) curves for each final model in a single plot.\n",
    "4. Which of the above metrics best illuminate the difference (if any) in model performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ae38",
   "metadata": {},
   "source": [
    "## Part 7: Holdout Set Evaluation (Instructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your final models `gnb_final`, `svm_final`, and `rf_final` will be evaluated on a holdout set by the instructor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
